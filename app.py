import streamlit as st
import cv2
from PIL import Image
import numpy as np
from ultralytics import YOLO, RTDETR
import tempfile
import os
import time
from streamlit_webrtc import webrtc_streamer, VideoProcessorBase
import av
import torch

# Detect and set device
@st.cache_resource
def get_device():
    if torch.cuda.is_available():
        device = torch.device("cuda")
        return device, "CUDA"
    elif torch.backends.mps.is_available():
        device = torch.device("mps")
        return device, "MPS"
    else:
        device = torch.device("cpu")
        return device, "CPU"

# Define colors for each class (BGR format)
CLASS_COLORS = {
    'Person': (255, 255, 255),      # White
    'gloves': (0, 128, 0),          # Green
    'goggles': (0, 128, 0),         # Green
    'helmet': (0, 128, 0),          # Green
    'no_gloves': (0, 0, 255),       # Red (BGR format)
    'no_goggle': (0, 0, 255),       # Red (BGR format)
    'no_helmet': (0, 0, 255)        # Red (BGR format)
}

# Load models based on selection
@st.cache_resource
def load_yolo_model(model_path):
    device, device_name = get_device()
    model = YOLO(model_path)
    model.to(device)
    print(f"YOLO model loaded on device: {device}")
    return model, device, device_name

@st.cache_resource
def load_rtdetr_model(model_path):
    device, device_name = get_device()
    model = RTDETR(model_path)
    model.to(device)
    print(f"RT-DETR model loaded on device: {device}")
    return model, device, device_name

st.title("ê°œì¸ë³´í˜¸ì¥ë¹„(PPE) ê°ì§€ ì‹œìŠ¤í…œ")
st.write("ì ì ˆí•œ ê°œì¸ ë³´í˜¸ ì¥ë¹„ì˜ ì°©ìš© ì—¬ë¶€ë¥¼ ê°ì§€í•©ë‹ˆë‹¤.")

# Model selection in sidebar (outside of modes)
st.sidebar.header("ëª¨ë¸ ì„ íƒ")
model_type = st.sidebar.radio(
    "ê°ì§€ ëª¨ë¸ ì„ íƒ:",
    ["YOLOv11", "RT-DETR v1", "YOLOv11 vs RT-DETR v1"],
)

# Model paths

yolo_model_path = '/Users/abc/Downloads/yolo-v11-s_E50_BS32_Baseline_mAP_0.777.pt'  # Replace with your YOLO model path
rtdetr_model_path = '/Users/abc/Downloads/best_rtdetr_x_oct23_1019am.pt'  # Replace with your RT-DETR model path

# Load selected model(s)
if model_type == "YOLOv11":
    model, device, device_name = load_yolo_model(yolo_model_path)
    st.sidebar.info(f"ğŸ“¦ ë¡œë“œë¨: YOLOv11")
    comparison_mode = False
elif model_type == "RT-DETR v1":
    model, device, device_name = load_rtdetr_model(rtdetr_model_path)
    st.sidebar.info(f"ğŸ“¦ ë¡œë“œë¨: RT-DETR v1")
    comparison_mode = False
else:  # Comparison mode
    yolo_model, yolo_device, yolo_device_name = load_yolo_model(yolo_model_path)
    rtdetr_model, rtdetr_device, rtdetr_device_name = load_rtdetr_model(rtdetr_model_path)
    model = yolo_model  # Default for non-image modes
    device = yolo_device
    device_name = yolo_device_name
    st.sidebar.info(f"ğŸ“¦ ë¡œë“œë¨: ë‘ ëª¨ë¸ ëª¨ë‘")
    comparison_mode = True

# Display device info in small grey text
st.sidebar.markdown(f"<p style='font-size:11px; color:#666666;'>{device_name} ì‚¬ìš© ì¤‘</p>", unsafe_allow_html=True)

st.sidebar.markdown("---")

# PPE Selection
st.sidebar.header("PPE í•­ëª© ì„ íƒ")
st.sidebar.write("ê°ì§€í•  PPE í•­ëª©ì„ ì„ íƒí•˜ì„¸ìš”:")

detect_helmet = st.sidebar.checkbox("í—¬ë©§ (Helmet)", value=True)
detect_goggles = st.sidebar.checkbox("ê³ ê¸€ (Goggles)", value=True)
detect_gloves = st.sidebar.checkbox("ì¥ê°‘ (Gloves)", value=True)

# Create a set of classes to detect based on selections
classes_to_detect = {'Person'}  # Always detect Person
if detect_helmet:
    classes_to_detect.update(['helmet', 'no_helmet'])
if detect_goggles:
    classes_to_detect.update(['goggles', 'no_goggle'])
if detect_gloves:
    classes_to_detect.update(['gloves', 'no_gloves'])

st.sidebar.markdown("---")

# Function to draw custom colored bounding boxes
def draw_detections(frame, results, classes_to_detect, detection_model, is_bgr=True):
    # For webcam/video: frame is already BGR, for images: frame is RGB
    if is_bgr:
        annotated_frame = frame.copy()
    else:
        # Convert RGB to BGR for processing
        annotated_frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)
    
    for box in results[0].boxes:
        x1, y1, x2, y2 = map(int, box.xyxy[0])
        conf = float(box.conf)
        cls = int(box.cls)
        label = detection_model.names[cls]
        
        # Skip if this class is not selected for detection
        if label not in classes_to_detect:
            continue
        
        # Get color for this class (BGR format)
        color = CLASS_COLORS.get(label, (0, 255, 0))  # Default to green if not found
        
        # Draw bounding box
        cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), color, 2)
        
        # Draw label background
        label_text = f'{label} {conf:.2f}'
        (text_width, text_height), baseline = cv2.getTextSize(label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)
        cv2.rectangle(annotated_frame, (x1, y1 - text_height - baseline - 5), (x1 + text_width, y1), color, -1)
        
        # Draw label text
        cv2.putText(annotated_frame, label_text, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 2)
    
    # Convert back to RGB only for image display
    if not is_bgr:
        annotated_frame = cv2.cvtColor(annotated_frame, cv2.COLOR_BGR2RGB)
    
    return annotated_frame

# Sidebar for mode selection
mode = st.sidebar.selectbox("ëª¨ë“œ ì„ íƒ", ["ì´ë¯¸ì§€ ì—…ë¡œë“œ", "ë™ì˜ìƒ ì—…ë¡œë“œ", "ì›¹ìº  (ì‹¤ì‹œê°„)"])

if mode == "ì´ë¯¸ì§€ ì—…ë¡œë“œ":
    uploaded_file = st.file_uploader("ì´ë¯¸ì§€ ì—…ë¡œë“œ", type=['jpg', 'jpeg', 'png'])
    
    if uploaded_file is not None:
        # Convert to OpenCV format
        image = Image.open(uploaded_file)
        img_array = np.array(image)
        
        if comparison_mode:
            # Comparison mode: run both models
            st.subheader("ëª¨ë¸ ë¹„êµ")
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.write("**YOLOv11 ê°ì§€**")
                with torch.no_grad():
                    yolo_results = yolo_model(img_array, device=yolo_device)
                yolo_annotated = draw_detections(img_array, yolo_results, classes_to_detect, yolo_model, is_bgr=False)
                st.image(yolo_annotated, caption="YOLOv11 ê²°ê³¼", use_container_width=True)
                
                st.write("ê°ì§€ ê²°ê³¼:")
                for box in yolo_results[0].boxes:
                    class_name = yolo_model.names[int(box.cls)]
                    if class_name in classes_to_detect:
                        confidence = float(box.conf)
                        st.write(f"- {class_name} ({confidence:.2f})")
            
            with col2:
                st.write("**RT-DETR v1 ê°ì§€**")
                with torch.no_grad():
                    rtdetr_results = rtdetr_model(img_array, device=rtdetr_device)
                rtdetr_annotated = draw_detections(img_array, rtdetr_results, classes_to_detect, rtdetr_model, is_bgr=False)
                st.image(rtdetr_annotated, caption="RT-DETR v1 ê²°ê³¼", use_container_width=True)
                
                st.write("ê°ì§€ ê²°ê³¼:")
                for box in rtdetr_results[0].boxes:
                    class_name = rtdetr_model.names[int(box.cls)]
                    if class_name in classes_to_detect:
                        confidence = float(box.conf)
                        st.write(f"- {class_name} ({confidence:.2f})")
        
        else:
            # Single model mode
            with torch.no_grad():
                results = model(img_array, device=device)
            
            # Draw custom colored bounding boxes
            annotated_img = draw_detections(img_array, results, classes_to_detect, model, is_bgr=False)
            
            # Display results (image is now in RGB format)
            st.image(annotated_img, caption="ê°ì§€ ê²°ê³¼")
            
            # Show detection details
            st.subheader("ê°ì§€ ê²°ê³¼:")
            for box in results[0].boxes:
                class_name = model.names[int(box.cls)]
                if class_name in classes_to_detect:
                    confidence = float(box.conf)
                    st.write(f"- {class_name} (ì‹ ë¢°ë„: {confidence:.2f})")

elif mode == "ë™ì˜ìƒ ì—…ë¡œë“œ":
    uploaded_video = st.file_uploader("ë™ì˜ìƒ ì—…ë¡œë“œ", type=['mp4', 'avi', 'mov'])
    
    if uploaded_video is not None:
        # Save uploaded video to temporary file
        input_path = "input_video.mp4"
        with open(input_path, "wb") as f:
            f.write(uploaded_video.read())
        
        st.success("âœ… ë™ì˜ìƒì´ ì„±ê³µì ìœ¼ë¡œ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤!")
        
        # Processing options
        st.write("**ì²˜ë¦¬ ì˜µì…˜:**")
        process_every_n = st.selectbox(
            "N í”„ë ˆì„ë§ˆë‹¤ ì²˜ë¦¬ (1 = ëª¨ë“  í”„ë ˆì„, ë†’ì„ìˆ˜ë¡ ë¹ ë¥¸ ì²˜ë¦¬):",
            [1, 2, 3, 5, 10],
            index=0
        )
        
        # Process video button
        if st.button("ğŸ¬ PPE ê°ì§€ë¡œ ë™ì˜ìƒ ì²˜ë¦¬"):
            temp_output = "temp_output.avi"
            output_path = "output_video.mp4"
            
            # Open video
            cap = cv2.VideoCapture(input_path)
            
            # Get video properties
            fps = int(cap.get(cv2.CAP_PROP_FPS))
            width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            
            # First save as AVI (more reliable)
            fourcc = cv2.VideoWriter_fourcc(*'XVID')
            out = cv2.VideoWriter(temp_output, fourcc, fps, (width, height))
            
            # Progress indicators
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            frame_count = 0
            last_detection_result = None
            
            st.info(f"ğŸ¥ {device_name}ì„(ë¥¼) ì‚¬ìš©í•˜ì—¬ {fps} FPSë¡œ {total_frames}ê°œ í”„ë ˆì„ ì²˜ë¦¬ ì¤‘...")
            
            while cap.isOpened():
                ret, frame = cap.read()
                if not ret:
                    break
                
                # Run detection on selected frames with explicit device
                if frame_count % process_every_n == 0:
                    with torch.no_grad():
                        results = model(frame, device=device)
                    annotated_frame = draw_detections(frame, results, classes_to_detect, model, is_bgr=True)
                    last_detection_result = annotated_frame
                else:
                    # Use last detection result or original frame
                    annotated_frame = last_detection_result if last_detection_result is not None else frame
                
                # Write frame to output video
                out.write(annotated_frame)
                
                # Update progress
                frame_count += 1
                progress = frame_count / total_frames
                progress_bar.progress(progress)
                status_text.text(f"í”„ë ˆì„ {frame_count}/{total_frames} ì²˜ë¦¬ ì¤‘ ({progress*100:.1f}%)")
            
            # Release resources
            cap.release()
            out.release()
            
            # Convert to web-compatible MP4 using ffmpeg if available
            status_text.text("ì›¹ í˜¸í™˜ í˜•ì‹ìœ¼ë¡œ ë³€í™˜ ì¤‘...")
            try:
                import subprocess
                subprocess.run([
                    'ffmpeg', '-i', temp_output, '-c:v', 'libx264', 
                    '-preset', 'fast', '-crf', '22', '-y', output_path
                ], check=True, capture_output=True)
                os.remove(temp_output)
            except (subprocess.CalledProcessError, FileNotFoundError):
                # If ffmpeg not available, try direct H.264 encoding
                status_text.text("ffmpegë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ëŒ€ì²´ ë°©ë²•ì„ ì‹œë„í•©ë‹ˆë‹¤...")
                cap2 = cv2.VideoCapture(temp_output)
                fourcc2 = cv2.VideoWriter_fourcc(*'avc1')
                out2 = cv2.VideoWriter(output_path, fourcc2, fps, (width, height))
                
                while cap2.isOpened():
                    ret, frame = cap2.read()
                    if not ret:
                        break
                    out2.write(frame)
                
                cap2.release()
                out2.release()
                os.remove(temp_output)
            
            # Clear progress indicators
            progress_bar.empty()
            status_text.empty()
            
            st.success("âœ… ë™ì˜ìƒ ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
            
            # Store processed video path in session state
            st.session_state.processed_video_path = output_path
        
        # Display processed video
        if 'processed_video_path' in st.session_state and os.path.exists(st.session_state.processed_video_path):
            st.write("---")
            st.write("**PPE ê°ì§€ê°€ ì ìš©ëœ ì²˜ë¦¬ëœ ë™ì˜ìƒ:**")
            
            # Display video with native HTML5 controls (play, pause, seek, speed control)
            st.video(st.session_state.processed_video_path)
            
            st.info("ğŸ’¡ íŒ: ë™ì˜ìƒ ì»¨íŠ¸ë¡¤ì„ ì‚¬ìš©í•˜ì—¬ ì¬ìƒ, ì¼ì‹œì •ì§€, íƒìƒ‰ ë° ì¬ìƒ ì†ë„ë¥¼ ì¡°ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë™ì˜ìƒì€ ëª¨ë“  ê°ì§€ ê²°ê³¼ê°€ í‘œì‹œëœ ìƒíƒœë¡œ ì „ì²´ ì†ë„ë¡œ ë¶€ë“œëŸ½ê²Œ ì¬ìƒë©ë‹ˆë‹¤!")
            
            # Download button
            with open(st.session_state.processed_video_path, "rb") as file:
                st.download_button(
                    label="ğŸ“¥ ì²˜ë¦¬ëœ ë™ì˜ìƒ ë‹¤ìš´ë¡œë“œ",
                    data=file,
                    file_name="ppe_detection_output.mp4",
                    mime="video/mp4"
                )

elif mode == "ì›¹ìº  (ì‹¤ì‹œê°„)":
    st.write("WebRTCë¥¼ ì‚¬ìš©í•œ ì‹¤ì‹œê°„ ì›¹ìº  ê°ì§€")
    
    if comparison_mode:
        # Comparison mode: display both models side by side
        st.subheader("ì‹¤ì‹œê°„ ëª¨ë¸ ë¹„êµ")
        
        # Single button to start both webcams
        start_comparison = st.checkbox("ğŸ¥ ë‘ ëª¨ë¸ ë™ì‹œ ì‹¤í–‰", value=False, key="start_both_webcams")
        
        if start_comparison:
            col1, col2 = st.columns(2)
            
            # Define the video processor class for YOLO
            class YOLODetector(VideoProcessorBase):
                def recv(self, frame):
                    img = frame.to_ndarray(format="bgr24")
                    
                    # Run detection with explicit device and no_grad for efficiency
                    with torch.no_grad():
                        results = yolo_model(img, device=yolo_device)
                    annotated_img = draw_detections(img, results, classes_to_detect, yolo_model, is_bgr=True)
                    
                    return av.VideoFrame.from_ndarray(annotated_img, format="bgr24")
            
            # Define the video processor class for RT-DETR
            class RTDETRDetector(VideoProcessorBase):
                def recv(self, frame):
                    img = frame.to_ndarray(format="bgr24")
                    
                    # Run detection with explicit device and no_grad for efficiency
                    with torch.no_grad():
                        results = rtdetr_model(img, device=rtdetr_device)
                    annotated_img = draw_detections(img, results, classes_to_detect, rtdetr_model, is_bgr=True)
                    
                    return av.VideoFrame.from_ndarray(annotated_img, format="bgr24")
            
            with col1:
                st.markdown("### YOLOv11")
                # Start YOLO webcam stream with custom styling
                ctx1 = webrtc_streamer(
                    key="yolo-detection",
                    video_processor_factory=YOLODetector,
                    rtc_configuration={"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]},
                    media_stream_constraints={"video": {"width": {"ideal": 1280}, "height": {"ideal": 720}}, "audio": False}
                )
            
            with col2:
                st.markdown("### RT-DETR v1")
                # Start RT-DETR webcam stream with custom styling
                ctx2 = webrtc_streamer(
                    key="rtdetr-detection",
                    video_processor_factory=RTDETRDetector,
                    rtc_configuration={"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]},
                    media_stream_constraints={"video": {"width": {"ideal": 1280}, "height": {"ideal": 720}}, "audio": False}
                )
        else:
            st.info("â¬†ï¸ ìœ„ì˜ ì²´í¬ë°•ìŠ¤ë¥¼ ì„ íƒí•˜ì—¬ ë‘ ëª¨ë¸ì˜ ì‹¤ì‹œê°„ ë¹„êµë¥¼ ì‹œì‘í•˜ì„¸ìš”.")
    
    else:
        # Single model mode
        st.info("ğŸ’¡ ì‹¤ì‹œê°„ ì›¹ìº  ê°ì§€ë¥¼ ì‹œì‘í•˜ë ¤ë©´ ì•„ë˜ ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”.")
        
        # Define the video processor class
        class PPEDetector(VideoProcessorBase):
            def recv(self, frame):
                img = frame.to_ndarray(format="bgr24")
                
                # Run detection with explicit device and no_grad for efficiency
                with torch.no_grad():
                    results = model(img, device=device)
                annotated_img = draw_detections(img, results, classes_to_detect, model, is_bgr=True)
                
                return av.VideoFrame.from_ndarray(annotated_img, format="bgr24")
        
        # Start webcam stream with larger resolution
        webrtc_streamer(
            key="ppe-detection",
            video_processor_factory=PPEDetector,
            rtc_configuration={"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]},
            media_stream_constraints={"video": {"width": {"ideal": 1280}, "height": {"ideal": 720}}, "audio": False}
        )
